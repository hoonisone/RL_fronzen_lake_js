{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8727faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from util import *\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3170cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionTauTable:\n",
    "    \"\"\"\n",
    "        state, action 쌍에 대한 tau값 관리\n",
    "    \"\"\"\n",
    "    def __init__(self, states, actions):\n",
    "        self.value_table = np.zeros((len(states), len(actions)), dtype=int)\n",
    "\n",
    "    def update(self, state, action):\n",
    "        # 전체 경우의 수에 값을 1씩 더하고 인자로 받은 케이스는 0으로 초기화\n",
    "        self.value_table = self.value_table+1\n",
    "        self.value_table[state, action] = 0\n",
    "\n",
    "    def get_taus_for_state(self, state):\n",
    "        return self.value_table[state]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8e4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(keys, values):\n",
    "    max_len = max([len(str(key)) for key in keys])\n",
    "    infor = [f\"%-{max_len}s: %s\" % (str(key), str(value)) for (key, value) in zip(keys, values)]\n",
    "    return \"\\n\".join(infor)\n",
    "    \n",
    "class GaussianDistributionModel:\n",
    "    MIN_STD = 0.0000001\n",
    "    \n",
    "    \n",
    "    def __init__(self, mean=0, variance=1, min_step_size = 0.05):\n",
    "        self.mean = mean\n",
    "        self.variance = variance\n",
    "        self.min_step_size = min_step_size\n",
    "        self.size = 0\n",
    "\n",
    "    def update(self, sample):\n",
    "        self.size += 1\n",
    "        step_size = self.step_size()\n",
    "        self.variance += step_size*((sample-self.mean)**2 - self.variance)\n",
    "        self.mean += step_size*(sample-self.mean)\n",
    "\n",
    "    def forget(self, ratio):\n",
    "        self.size = math.floor(self.size*(1-ratio))\n",
    "    \n",
    "    def step_size(self):\n",
    "        if(self.size == 0):\n",
    "            return 1\n",
    "        return max(1/self.size, self.min_step_size)\n",
    "    \n",
    "    def get_infor(self):\n",
    "        keys = [\"Mean\", \"Variance\", \"Size\", \"Step_size\", \"Min_step_size\"]\n",
    "        values = [self.mean, self.variance, self.size, self.step_size(), self.min_step_size]\n",
    "        return print_dict(keys, values)\n",
    "                \n",
    "model = GaussianDistributionModel()\n",
    "\n",
    "model.update(1)\n",
    "assert(model.mean == 1)\n",
    "assert(model.variance == 1)\n",
    "\n",
    "model.update(3)\n",
    "assert(model.mean == 2)\n",
    "assert(model.variance == 2.5)\n",
    "\n",
    "model.update(5)\n",
    "assert(model.mean == 3)\n",
    "assert near(4.666666666666666, model.variance, 1e-5)\n",
    "\n",
    "model.update(3)\n",
    "\n",
    "assert(model.mean == 3)\n",
    "assert near(3.5, model.variance, 1e-5)\n",
    "\n",
    "model.update(1)\n",
    "assert(model.mean == 2.6)\n",
    "assert near(3.6, model.variance, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4b80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel:\n",
    "    def __init__(self):\n",
    "        self.rewards= []\n",
    "        self.sum = 0\n",
    "    \n",
    "    def add(self, reward):\n",
    "        self.rewards.insert(0, reward)\n",
    "        self.sum += reward\n",
    "\n",
    "    def pop(self):\n",
    "        reward = self.rewards.pop()\n",
    "        self.sum -= reward\n",
    "        return reward\n",
    "    \n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.sum/len(self.rewards)\n",
    "    \n",
    "model = RewardModel()\n",
    "\n",
    "model.add(2)\n",
    "assert (model.mean == 2)\n",
    "\n",
    "model.add(4)\n",
    "assert (model.mean == 3)\n",
    "\n",
    "model.add(6)\n",
    "assert (model.mean == 4)\n",
    "\n",
    "model.pop()\n",
    "assert (model.mean == 5)\n",
    "\n",
    "model.pop()\n",
    "assert (model.mean == 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3acaa424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardStateModel:\n",
    "    def __init__(self, buffer_size=10):\n",
    "        self.state_to_reward_model = {}\n",
    "        self.sample_buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "\n",
    "    def update(self, reward, next_state, finished):\n",
    "        if next_state not in self.state_to_reward_model:\n",
    "            self.state_to_reward_model[next_state] = RewardModel()\n",
    "        \n",
    "        self.sample_buffer.insert(0, [next_state, finished])\n",
    "        self.state_to_reward_model[next_state].add(reward)\n",
    "        \n",
    "        if self.buffer_size < len(self.sample_buffer):\n",
    "            return_next_state, return_finished = self.sample_buffer.pop()\n",
    "            return_reward = self.state_to_reward_model[return_next_state].pop()\n",
    "            return [return_reward, return_next_state, return_finished]\n",
    "        \n",
    "        return [None, None, None]\n",
    "    \n",
    "\n",
    "    def get_all_next_states(self):\n",
    "        return list(self.state_to_reward_model.keys())\n",
    "        \n",
    "\n",
    "    def get_sample(self, reward, next_state, finished):\n",
    "        if self.check(False, False, False, reward, next_state, finished):\n",
    "            if len(self.sample_buffer) == 0:\n",
    "                return [None, None, None]\n",
    "            \n",
    "            [next_state, finished] = self.sample_buffer[np.random.choice(len(self.sample_buffer))]\n",
    "#             random_util.randomChoiceself.sample_buffer()\n",
    "            return self.get_sample(None, next_state, finished)\n",
    "        elif self.check(False, True, True, reward, next_state, finished): # next_state, finished -> reward 만\n",
    "            if next_state not in self.state_to_reward_model:\n",
    "                return [None, None, None]\n",
    "            \n",
    "            reward = self.state_to_reward_model[next_state].mean\n",
    "        \n",
    "        elif self.check(False, True, False, reward, next_state, finished): # next_state, finished -> reward 만\n",
    "            if next_state not in self.state_to_reward_model:\n",
    "                return [None, None, None]\n",
    "            \n",
    "            finished = False\n",
    "            for ns, _f in self.sample_buffer:\n",
    "                if ns == next_state:\n",
    "                    finished = _f\n",
    "                    break\n",
    "                    \n",
    "            return self.get_sample(None, next_state, finished)\n",
    "\n",
    "        return [reward, next_state, finished]\n",
    "    \n",
    "\n",
    "    def check(self, a1, b1, c1, a2, b2, c2):\n",
    "        return (a1 == (a2!=None) \n",
    "        and b1 == (b2 != None) \n",
    "        and c1 == (c2 != None))\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.sample_buffer)\n",
    "\n",
    "    def forget(self, forget_ratio):\n",
    "        forget_num = math.floor(self.size*forget_ratio)\n",
    "        for i in range(forget_num): \n",
    "            forget_next_state, forget_finished = self.sample_buffer.pop()\n",
    "            self.state_to_reward_model[forget_next_state].pop()\n",
    "            \n",
    "            \n",
    "model = RewardStateModel()\n",
    "model.update(0, \"A\", False)\n",
    "model.update(1, \"A\", False)\n",
    "model.update(2, \"A\", False)\n",
    "model.update(5, \"B\", True)\n",
    "model.update(6, \"B\", True)\n",
    "model.update(7, \"B\", True)\n",
    "(r, ns, f) = model.get_sample(None, \"A\", False)\n",
    "assert (r == 1)\n",
    "assert (ns == \"A\")\n",
    "assert (f == False)\n",
    "\n",
    "(r, ns, f) = model.get_sample(None, \"B\", None)\n",
    "assert (r == 6)\n",
    "assert (ns == \"B\")\n",
    "assert (f == True)\n",
    "\n",
    "assert (model.get_all_next_states() == [\"A\", \"B\"])\n",
    "\n",
    "model.forget(2/3)\n",
    "assert (model.size == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee0bca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableRewardStateModel:\n",
    "    def __init__(self, recent_buffer_size=10, old_buffer_size=100):\n",
    "        self.recent_sample_model = RewardStateModel(buffer_size=recent_buffer_size)\n",
    "        self.old_sample_model = RewardStateModel(buffer_size=old_buffer_size)\n",
    "    \n",
    "    def update(self, reward, next_state, finished):\n",
    "        pop_state, pop_action, pop_finished = self.recent_sample_model.update(reward, next_state, finished)\n",
    "        if pop_state != None:\n",
    "            self.old_sample_model.update(pop_state, pop_action, pop_finished)\n",
    "        \n",
    "    @property\n",
    "    def recent_size(self):\n",
    "        return self.recent_sample_model.size\n",
    "    \n",
    "    @property\n",
    "    def old_size(self):\n",
    "        return self.old_sample_model.size\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.recent_size + self.old_size\n",
    "    \n",
    "    def get_all_next_states(self):\n",
    "        arr1 = self.recent_sample_model.get_all_next_states()\n",
    "        arr2 = self.old_sample_model.get_all_next_states()\n",
    "        return arr1.concat(arr2)\n",
    "    \n",
    "\n",
    "    def is_empty(self):\n",
    "        return self.size == 0\n",
    "\n",
    "    def get_sample(self, type=None, reward=None, next_state=None, finished=None):\n",
    "        if(type == None):\n",
    "            recent_ratio = self.recent_size/self.size\n",
    "            recent = (random.random() <= recent_ratio)\n",
    "            type = \"recent\" if recent else \"old\"\n",
    "            return self.get_sample(type, reward, next_state, finished)\n",
    "        else:\n",
    "            \n",
    "            model = self.recent_sample_model if (type == \"recent\") else self.old_sample_model\n",
    "            result = model.get_sample(None, None, None)\n",
    "            reward, next_state, finished = result\n",
    "            if reward == None:\n",
    "                return [None, None, None, None]\n",
    "            return [type, reward, next_state, finished]\n",
    "        \n",
    "    def check(self, a1, b1, c1, d1, a2, b2, c2, d2):\n",
    "        return all([a1 == (a2!=None), \n",
    "                    b1 == (b2 != None), \n",
    "                    c1 == (c2 != None),\n",
    "                    d1 == (d2 != None)])\n",
    "    \n",
    "\n",
    "\n",
    "    def forget_recent_samples(self, forget_ratio):\n",
    "        self.recent_sample_model.forget(forget_ratio)\n",
    "    \n",
    "    def forget_old_samples(self, forget_ratio):\n",
    "        self.old_sample_model.forget(forget_ratio)\n",
    "    \n",
    "\n",
    "    def is_separable(self):\n",
    "        return self.recent_sample_model.buffer_size<=self.old_sample_model.size\n",
    "    \n",
    "model = SeparableRewardStateModel()\n",
    "model.update(0, \"A\", False)\n",
    "\n",
    "model.forget_recent_samples(1)\n",
    "model.forget_recent_samples(1)\n",
    "\n",
    "assert(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c7e3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModel:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def update(self, sample):\n",
    "        self.buffer.insert(0, sample)\n",
    "        if(self.buffer_size < len(self.buffer)):\n",
    "            self.buffer.pop()\n",
    "            \n",
    "    def get_sample(self):\n",
    "        if len(self.buffer) == 0:\n",
    "            return None\n",
    "        \n",
    "        return self.buffer[np.random.choice(range(len(self.buffer)))]\n",
    "    \n",
    "\n",
    "    def forget(self):\n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "model = SampleModel(3)\n",
    "model.update(1)\n",
    "\n",
    "assert (model.get_sample() == 1)\n",
    "model.update(2)\n",
    "model.update(3)\n",
    "model.update(4)\n",
    "\n",
    "assert(1 not in [model.get_sample() for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c819727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all([True, not 0, not 0, True, True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57e6514a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SeperableStateActionModel:\n",
    "    \n",
    "    def __init__(self, state_num, action_num, recent_buffer_size=10, old_buffer_size=10):\n",
    "        creator = lambda : SeparableRewardStateModel(recent_buffer_size=recent_buffer_size, old_buffer_size=old_buffer_size)\n",
    "        self.table = create_object_tensor([state_num, action_num], creator)\n",
    "        self.recent_visit = SampleModel(100000)\n",
    "        self.state_to_pre_state_action = {}\n",
    "        \n",
    "    def update(self, state, action, reward, next_state, finished):\n",
    "        self.recent_visit.update([state, action])\n",
    "\n",
    "        self.table[state][action].update(reward, next_state, finished)\n",
    "\n",
    "        if next_state not in self.state_to_pre_state_action:\n",
    "            self.state_to_pre_state_action[next_state] = set()\n",
    "        \n",
    "        self.state_to_pre_state_action[next_state].add(str([state, action]))\n",
    "    \n",
    "    def get_sample(self, type = None, state=None, action=None, reward=None, next_state=None, finished=None):\n",
    "        \n",
    "        if all([type==None, state==None, action==None, reward==None, next_state==None, finished==None]):\n",
    "            \n",
    "            sample = self.recent_visit.get_sample()\n",
    "            if sample == None:\n",
    "                return [None, None, None, None, None, None]\n",
    "            state, action = sample            \n",
    "            \n",
    "            return self.get_sample(None, state, action, None, None, None)\n",
    "        \n",
    "        elif all ([type==None, state!=None, action!=None, reward==None, next_state==None, finished==None]):\n",
    "\n",
    "            type, reward, next_state, finished = self.table[state][action].get_sample(None, None, None, None)\n",
    "            \n",
    "            if type == None:\n",
    "                return [None, None, None, None, None, None]\n",
    "            \n",
    "            return [type, state, action, reward, next_state, finished]\n",
    "        elif all ([type==None, state!=None, action!=None, reward==None, next_state!=None, finished==None]):\n",
    "            type, reward, next_state, finished = self.table[state][action].get_sample(None, None, next_state, None)\n",
    "            return [type, state, action, reward, next_state, finished]\n",
    "        \n",
    "        \n",
    "        raise Exception(\"정의되지 않은 케이스\");\n",
    "        \n",
    "    def get_all_samples(_type=None, state=None, action=None, reward=None, next_state=None, finished=None):\n",
    "        if all([_type==None, state==None, action==None, reward==None, next_state!=None, finished==None]):\n",
    "            return []\n",
    "        arr = []\n",
    "        if next_state in state_to_pre_state_action:\n",
    "            for x in state_to_pre_state_action[next_state]:\n",
    "                _state, _action = json.loads(x)\n",
    "                _type, _state, _action, _reward, _next_state, _finished = get_sample(\n",
    "                    None, _state, _action, None, next_state, None\n",
    "                )\n",
    "                if _type is None:\n",
    "                    continue\n",
    "                arr.append([_type, _state, _action, _reward, _next_state, _finished])\n",
    "\n",
    "        return arr\n",
    "\n",
    "    def forget_visit_list_by_state_action(self, state, action):\n",
    "        new_list = []\n",
    "        for visit in self.recent_visit:\n",
    "            if visit[0] == state and visit[1] == action:\n",
    "                continue\n",
    "            new_list.insert(0, visit)\n",
    "\n",
    "        self.recent_visit.forget()\n",
    "        \n",
    "        for visit in new_list:\n",
    "            self.recent_visit.update(visit)\n",
    "        \n",
    "    def forget_by_state_action(self, state, action, forget_ratio):\n",
    "        self.table[state][action].forget_old_samples(forget_ratio)\n",
    "    \n",
    "    def forget_pre_state_action_by_state(self, state, action, forget_ratio):\n",
    "        next_states = self.table[state][action].get_all_next_states()\n",
    "        for next_state in next_states:\n",
    "            if random.random() < forget_ratio:\n",
    "                pre_state_action_str = json.dumps([state, action])\n",
    "                if next_state in self.state_to_pre_state_action:\n",
    "                    self.state_to_pre_state_action[next_state].discard(pre_state_action_str)\n",
    "\n",
    "model = SeperableStateActionModel(3, 3)\n",
    "\n",
    "# model.update(1, 2, 0, 1, False)\n",
    "model.get_sample(None, None, None, None, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab108bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StateActionValue at 0x1b1e74387f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StateActionValue():\n",
    "    def __init__(self, state_num, action_num, mean=0, variance=1, min_step_size=0.05):\n",
    "        creator = lambda: GaussianDistributionModel(mean=mean, variance=variance, min_step_size=min_step_size)\n",
    "        self.table = create_object_tensor([state_num, action_num], creator)\n",
    "        \n",
    "        \n",
    "    def update(self, state, action, value):\n",
    "        self.table[state][action].update(value)\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self.table[state][action].mean\n",
    "    \n",
    "    def get_variance(self, state, action):\n",
    "        return self.table[state][action].variance\n",
    "    \n",
    "\n",
    "    def get_size(self, state, action):\n",
    "        return self.table[state][action].size\n",
    "    \n",
    "    \n",
    "StateActionValue(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15431dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ab06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
