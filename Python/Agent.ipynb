{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7704b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Policy.ipynb\n",
      "importing Jupyter notebook from Model.ipynb\n",
      "importing Jupyter notebook from util.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import random\n",
    "import Policy\n",
    "import Model\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77eb7590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 232\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforget_history\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_episode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_step, state])\n\u001b[0;32m    231\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent([\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 232\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36mAgent.start\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_action\n",
      "Cell \u001b[1;32mIn[2], line 196\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    194\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action_values_for_state(state)\n\u001b[0;32m    195\u001b[0m tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tau_values_for_state(state)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:97\u001b[0m, in \u001b[0;36mchoose_action\u001b[1;34m(self, actions, q_values, tau)\u001b[0m\n",
      "File \u001b[1;32m<string>:88\u001b[0m, in \u001b[0;36mepsilon_greedy_choose\u001b[1;34m(self, actions, values)\u001b[0m\n",
      "File \u001b[1;32m<string>:77\u001b[0m, in \u001b[0;36mgreedy_choose\u001b[1;34m(self, actions, values)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, states, actions):\n",
    "        # reward\n",
    "        self.default_reward = -0.03\n",
    "        self.curiosity_reward = 0.001\n",
    "        self.repeat_penalty = -0.01\n",
    "\n",
    "        # basic element\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.state_num = len(states)\n",
    "        self.action_num = len(actions)\n",
    "\n",
    "        self.past_state = None\n",
    "        self.past_action = None\n",
    "\n",
    "        self.finished = True\n",
    "\n",
    "        # Policy\n",
    "        self.policy = Policy.Policy(0.05, 0.0001)\n",
    "\n",
    "        min_step_size = 0.05\n",
    "        recent_buffer_size = 10\n",
    "        old_buffer_size = 30\n",
    "\n",
    "        print(recent_buffer_size, old_buffer_size)\n",
    "\n",
    "        self.value = Model.StateActionValue(state_num=self.state_num, action_num=self.action_num, min_step_size=min_step_size)\n",
    "        self.recent_value = Model.StateActionValue(state_num=self.state_num, action_num=self.action_num, min_step_size=min_step_size)\n",
    "        self.old_value = Model.StateActionValue(state_num=self.state_num, action_num=self.action_num, min_step_size=min_step_size)\n",
    "\n",
    "        self.model = Model.SeperableStateActionModel(state_num=self.state_num, action_num=self.action_num, recent_buffer_size=recent_buffer_size, old_buffer_size=old_buffer_size)\n",
    "\n",
    "        self.tau_value_table = Model.ActionTauTable(states, actions)\n",
    "\n",
    "#         self.after_action_value_update_callback = Callback_2()  # state, action\n",
    "\n",
    "#         self.after_step_callback = Callback_0()\n",
    "#         self.goal_callback = Callback_0()\n",
    "#         self.hall_callback = Callback_0()\n",
    "#         self.first_state = Callback_1()\n",
    "#         self.first_state_action = Callback_2()\n",
    "        self.total_step = 0\n",
    "        self.total_episode = 0\n",
    "\n",
    "        self.use_forget = False\n",
    "        self.planning_num = 100\n",
    "        self.heap = util.Heap()\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.planning_value_threshold = 0.1\n",
    "        self.forget_history = []\n",
    "\n",
    "    def set_epsilon(self, value):\n",
    "        self.policy.epsilon = value\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        return self.policy.epsilon\n",
    "\n",
    "    def set_kappa(self, value):\n",
    "        self.policy.kappa = value\n",
    "\n",
    "    def get_kappa(self):\n",
    "        return self.policy.kappa\n",
    "\n",
    "    def set_step_size(self, value):\n",
    "        self.action_state_value_manager.step_size = value\n",
    "\n",
    "    def get_step_size(self):\n",
    "        return self.action_state_value_manager.step_size\n",
    "\n",
    "    def get_state_e_value(self, state):\n",
    "        return self.action_state_value_manager.getStateValue(state)\n",
    "\n",
    "    def reset_all(self):\n",
    "        self.reset_all_value()\n",
    "        self.reset_all_model()\n",
    "\n",
    "    def reset_all_value(self):\n",
    "        self.action_state_value_manager.reset_all_value()\n",
    "\n",
    "    def reset_all_model(self):\n",
    "        self.action_state_value_manager.reset_all_model()\n",
    "\n",
    "    def set_group(self, group):\n",
    "        self.group = group\n",
    "\n",
    "    def start(self, state):\n",
    "        self.finished = False\n",
    "        self.total_episode += 1\n",
    "        self.past_state = state\n",
    "        self.past_action = self.choose_action(state)\n",
    "        return self.past_action\n",
    "\n",
    "    def check_recent_old_difference(self, state, action):\n",
    "        if self.is_distribution_comparable(state, action):\n",
    "            recent_mean = self.recent_value.get_value(state, action)\n",
    "            recent_variance = self.recent_value.get_variance(state, action)\n",
    "            old_mean = self.old_value.get_value(state, action)\n",
    "            old_variance = self.old_value.get_variance(state, action)\n",
    "\n",
    "            recent_z = abs(recent_mean - old_mean) / (old_variance ** 0.5)\n",
    "            old_z = abs(old_mean - recent_mean) / (recent_variance ** 0.5)\n",
    "\n",
    "            return min(recent_z, old_z)\n",
    "        return 0\n",
    "\n",
    "    def bootstrap_value(self, reward, next_state, finished):\n",
    "        next_return = 0 if finished else max(self.get_action_values_for_state(next_state))\n",
    "        cur_return = reward + self.gamma * next_return\n",
    "        return cur_return\n",
    "\n",
    "    def pq_planning(self):\n",
    "        visited = set()\n",
    "        while not self.heap.is_empty():\n",
    "            state, action = self.heap.pop()\n",
    "            type_, state, action, reward, next_state, finished = self.model.get_sample(None, state, action, None, None, None)\n",
    "            if type_ is None:\n",
    "                continue\n",
    "\n",
    "            visited.add(f\"{state},{action}\")\n",
    "\n",
    "            value = self.bootstrap_value(reward, next_state, finished)\n",
    "            self.value.update(state, action, value)\n",
    "            self.after_action_value_update_callback.invoke(state, action)\n",
    "\n",
    "            next_state = state\n",
    "            samples = self.model.get_all_samples(None, None, None, None, next_state, None)\n",
    "\n",
    "            for type_, state, action, reward, next_state, finished in samples:\n",
    "                if f\"{state},{action}\" in visited:\n",
    "                    continue\n",
    "                value = self.bootstrap_value(reward, next_state, finished)\n",
    "                p = abs(value - self.value.get_value(state, action))\n",
    "                if self.planning_value_threshold <= p:\n",
    "                    self.heap.push((state, action))\n",
    "    def planning(self):\n",
    "        for _ in range(self.planning_num):\n",
    "            type_, state, action, reward, next_state, finished = self.model.get_sample(None, None, None, None, None, None)\n",
    "            if type_ is None:\n",
    "                continue\n",
    "\n",
    "            value = self.bootstrap_value(reward, next_state, finished)\n",
    "            self.value.update(state, action, value)\n",
    "            if type_ == \"recent\":\n",
    "                self.recent_value.update(state, action, value)\n",
    "            elif type_ == \"old\":\n",
    "                self.old_value.update(state, action, value)\n",
    "\n",
    "            self.after_action_value_update_callback.invoke(state, action)\n",
    "\n",
    "    def _step(self, state, action, reward, next_state, finished):\n",
    "        value = self.bootstrap_value(reward, next_state, finished)\n",
    "\n",
    "        p = value - self.value.get_value(state, action)\n",
    "        if self.planning_value_threshold < p:\n",
    "            self.heap.push([state, action])\n",
    "\n",
    "        self.value.update(state, action, value)\n",
    "        self.after_action_value_update_callback.invoke(state, action)\n",
    "        self.recent_value.update(state, action, value)\n",
    "        self.model.update(state, action, reward, next_state, finished)\n",
    "\n",
    "        recent_old_difference = self.check_recent_old_difference(state, action)\n",
    "\n",
    "        if self.use_forget:\n",
    "            self.forget_model(state, action, next_state, recent_old_difference)\n",
    "\n",
    "        env_changed = 1 < recent_old_difference\n",
    "        self.policy.update_parameter(env_changed)\n",
    "\n",
    "        self.planning()\n",
    "\n",
    "    def step(self, reward, state, finished):\n",
    "        reward += self.default_reward\n",
    "\n",
    "        self._step(self.past_state, self.past_action, reward, state, finished)\n",
    "\n",
    "        self.past_state = state\n",
    "        self.past_action = self.choose_action(self.past_state)\n",
    "\n",
    "        # update tau\n",
    "        self.tau_value_table.update(self.past_state, self.past_action)\n",
    "\n",
    "        self.total_step += 1\n",
    "\n",
    "        self.after_step_callback.invoke()\n",
    "\n",
    "        # return action\n",
    "        self.finished = finished\n",
    "        return self.past_action\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        q_values = self.get_action_values_for_state(state)\n",
    "        tau = self.get_tau_values_for_state(state)\n",
    "        return self.policy.choose_action(self.actions, q_values, tau)\n",
    "\n",
    "    def get_action_values_for_state(self, state):\n",
    "        arr = []\n",
    "        for action in self.actions:\n",
    "            arr.append(self.value.get_value(state, action))\n",
    "        return arr\n",
    "\n",
    "    def get_tau_values_for_state(self, state):\n",
    "        return self.tau_value_table.value_table[state]\n",
    "\n",
    "    def get_state_value(self, state):\n",
    "        q_values = self.get_action_values_for_state(state)\n",
    "        return max(q_values)\n",
    "\n",
    "    def get_state_value_map(self):\n",
    "        value_map = []\n",
    "        for state in self.agent.states:\n",
    "            value_map.append(self.get_state_value(state))\n",
    "        return value_map\n",
    "\n",
    "    def is_distribution_comparable(self, state, action):\n",
    "        return (self.recent_value.get_size(state, action) > 10) and (self.old_value.get_size(state, action) > 10)\n",
    "\n",
    "    def forget_model(self, state, action, next_state, model_difference):\n",
    "        if model_difference > 2.3:\n",
    "            self.model.forget_by_state_action(state, action, 1)\n",
    "            # self.model.forget_by_state(next_state, 1)\n",
    "            print(\"forget\")\n",
    "        elif model_difference > 1.5:\n",
    "            self.model.forget_by_state_action(state, action, 1)\n",
    "            # self.model.forget_by_state(next_state, 0.5)\n",
    "            print(\"forget 0.2\")\n",
    "            self.forget_history.append([self.total_episode, self.total_step, state])\n",
    "\n",
    "agent = Agent([0], [0])\n",
    "agent.start(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32abd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868224f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
